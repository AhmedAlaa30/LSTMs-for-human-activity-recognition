{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5722068,"sourceType":"datasetVersion","datasetId":3290377}],"dockerImageVersionId":30761,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import necessary libraries\nimport os\nimport numpy as np\nimport random\nimport cv2\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical, plot_model\nfrom tensorflow.keras.layers import ConvLSTM2D, MaxPooling3D, TimeDistributed, Dropout, Flatten, Dense, Input\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, accuracy_score, f1_score, confusion_matrix\nimport datetime as dt\nimport warnings\n\n# Suppress TensorFlow warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Define constants\nIMAGE_HEIGHT, IMAGE_WIDTH = 64, 64\nSEQUENCE_LENGTH = 20\nDATASET_DIR = '/kaggle/input/human-activity-recognition-video-dataset/Human Activity Recognition - Video Dataset'\nCLASSES_LIST = ['Clapping', 'Meet and Split', 'Sitting', 'Standing Still', 'Walking', 'Walking While Reading Book', 'Walking While Using Phone']\n\ndef frames_extraction(video_path, sequence_length=SEQUENCE_LENGTH, image_height=IMAGE_HEIGHT, image_width=IMAGE_WIDTH):\n    \"\"\"Extract frames from a video, resizing and normalizing each frame.\"\"\"\n    frames_list = []\n    video_reader = cv2.VideoCapture(video_path)\n    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n    skip_frames_window = max(int(video_frames_count / sequence_length), 1)\n\n    for frame_counter in range(sequence_length):\n        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n        success, frame = video_reader.read()\n        if not success:\n            break\n        resized_frame = cv2.resize(frame, (image_width, image_height))\n        normalized_frame = resized_frame / 255.0\n        frames_list.append(normalized_frame)\n\n    video_reader.release()\n    return np.array(frames_list)\n\ndef create_dataset():\n    \"\"\"Extract frames from videos in each class and create datasets.\"\"\"\n    features = []\n    labels = []\n    video_files_paths = []\n\n    for class_index, class_name in enumerate(CLASSES_LIST):\n        print(f'Extracting Data of Class: {class_name}')\n        class_dir = os.path.join(DATASET_DIR, class_name)\n        if not os.path.exists(class_dir):\n            print(f\"Directory {class_dir} does not exist.\")\n            continue\n        files_list = os.listdir(class_dir)\n        for file_name in files_list:\n            video_file_path = os.path.join(class_dir, file_name)\n            frames = frames_extraction(video_file_path)\n            if len(frames) == SEQUENCE_LENGTH:\n                features.append(frames)\n                labels.append(class_index)\n                video_files_paths.append(video_file_path)\n\n    features = np.asarray(features)\n    labels = np.array(labels)\n    return features, labels, video_files_paths\n\ndef create_convlstm_model(sequence_length, image_height, image_width, num_classes):\n    \"\"\"Create a ConvLSTM model for video classification.\"\"\"\n    model = Sequential()\n    model.add(Input(shape=(sequence_length, image_height, image_width, 3)))\n    model.add(ConvLSTM2D(filters=4, kernel_size=(3, 3), activation='tanh', recurrent_dropout=0.2, return_sequences=True, data_format='channels_last'))\n    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same'))\n    model.add(TimeDistributed(Dropout(0.2)))\n    model.add(ConvLSTM2D(filters=8, kernel_size=(3, 3), activation='tanh', recurrent_dropout=0.2, return_sequences=True))\n    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same'))\n    model.add(TimeDistributed(Dropout(0.2)))\n    model.add(ConvLSTM2D(filters=14, kernel_size=(3, 3), activation='tanh', recurrent_dropout=0.2, return_sequences=True))\n    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same'))\n    model.add(TimeDistributed(Dropout(0.2)))\n    model.add(ConvLSTM2D(filters=16, kernel_size=(3, 3), activation='tanh', recurrent_dropout=0.2, return_sequences=True))\n    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same'))\n    model.add(TimeDistributed(Dropout(0.2)))\n    model.add(Flatten())\n    model.add(Dense(num_classes, activation='softmax'))\n\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    model.summary()\n    return model\n\n# Create the dataset\nfeatures, labels, video_files_paths = create_dataset()\n\n# Convert labels to one-hot encoding\none_hot_encoded_labels = to_categorical(labels)\n\n# Set random seeds for reproducibility\nseed_constant = 27\nnp.random.seed(seed_constant)\nrandom.seed(seed_constant)\ntf.random.set_seed(seed_constant)\n\n# Split the data into training (75%) and testing (25%) sets\nfeatures_train, features_test, labels_train, labels_test = train_test_split(features, one_hot_encoded_labels, test_size=0.25, shuffle=True, random_state=seed_constant)\n\n# Create the ConvLSTM model\nnum_classes = len(CLASSES_LIST)\nconvlstm_model = create_convlstm_model(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, num_classes)\n\n# Create an instance of Early Stopping Callback\nearly_stopping_callback = EarlyStopping(monitor='val_loss', patience=10, mode='min', restore_best_weights=True)\n\n# Train the model\nconvlstm_model_training_history = convlstm_model.fit(\n    x=features_train,\n    y=labels_train,\n    epochs=10,\n    batch_size=1,\n    shuffle=True,\n    validation_split=0.2,\n    callbacks=[early_stopping_callback]\n)\n\n# Evaluate the model\nmodel_evaluation_history = convlstm_model.evaluate(features_test, labels_test)\nmodel_evaluation_loss, model_evaluation_accuracy = model_evaluation_history\n\n# Define the date and time format for saving the model file\ndate_time_format = '%Y_%m_%d__%H_%M_%S'\ncurrent_date_time_dt = dt.datetime.now()\ncurrent_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)\n\n# Define the model file name\nmodel_file_name = f'convlstm_model___Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}_Accuracy_{model_evaluation_accuracy}.h5'\n\n# Save the model\nconvlstm_model.save(model_file_name)\n\n# Print a success message\nprint(\"Model Created and Saved Successfully!\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-29T14:54:25.277467Z","iopub.execute_input":"2024-08-29T14:54:25.277950Z","iopub.status.idle":"2024-08-29T17:35:31.985033Z","shell.execute_reply.started":"2024-08-29T14:54:25.277905Z","shell.execute_reply":"2024-08-29T17:35:31.983497Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stdout","text":"Extracting Data of Class: Clapping\nExtracting Data of Class: Meet and Split\nExtracting Data of Class: Sitting\nExtracting Data of Class: Standing Still\nExtracting Data of Class: Walking\nExtracting Data of Class: Walking While Reading Book\nExtracting Data of Class: Walking While Using Phone\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ conv_lstm2d_4 (\u001b[38;5;33mConvLSTM2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m4\u001b[0m)  │         \u001b[38;5;34m1,024\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling3d_4 (\u001b[38;5;33mMaxPooling3D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m4\u001b[0m)  │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ time_distributed_4              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m4\u001b[0m)  │             \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv_lstm2d_5 (\u001b[38;5;33mConvLSTM2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m8\u001b[0m)  │         \u001b[38;5;34m3,488\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling3d_5 (\u001b[38;5;33mMaxPooling3D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m8\u001b[0m)  │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ time_distributed_5              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m8\u001b[0m)  │             \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv_lstm2d_6 (\u001b[38;5;33mConvLSTM2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m14\u001b[0m) │        \u001b[38;5;34m11,144\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling3d_6 (\u001b[38;5;33mMaxPooling3D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m14\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ time_distributed_6              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m14\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv_lstm2d_7 (\u001b[38;5;33mConvLSTM2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │        \u001b[38;5;34m17,344\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling3d_7 (\u001b[38;5;33mMaxPooling3D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ time_distributed_7              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2880\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │        \u001b[38;5;34m20,167\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ conv_lstm2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvLSTM2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling3d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ time_distributed_4              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv_lstm2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvLSTM2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,488</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling3d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ time_distributed_5              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv_lstm2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvLSTM2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">11,144</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling3d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ time_distributed_6              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv_lstm2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvLSTM2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">17,344</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling3d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ time_distributed_7              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2880</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,167</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m53,167\u001b[0m (207.68 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">53,167</span> (207.68 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m53,167\u001b[0m (207.68 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">53,167</span> (207.68 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m667/667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 277ms/step - accuracy: 0.1722 - loss: 1.9687 - val_accuracy: 0.2036 - val_loss: 1.9469\nEpoch 2/10\n\u001b[1m667/667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 279ms/step - accuracy: 0.2005 - loss: 1.9194 - val_accuracy: 0.5389 - val_loss: 1.3592\nEpoch 3/10\n\u001b[1m667/667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 277ms/step - accuracy: 0.5969 - loss: 1.2270 - val_accuracy: 0.6946 - val_loss: 0.8301\nEpoch 4/10\n\u001b[1m667/667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 278ms/step - accuracy: 0.8365 - loss: 0.5274 - val_accuracy: 0.8263 - val_loss: 0.5974\nEpoch 5/10\n\u001b[1m667/667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 285ms/step - accuracy: 0.9327 - loss: 0.2331 - val_accuracy: 0.8623 - val_loss: 0.5301\nEpoch 6/10\n\u001b[1m667/667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 276ms/step - accuracy: 0.9207 - loss: 0.2123 - val_accuracy: 0.8802 - val_loss: 0.3479\nEpoch 7/10\n\u001b[1m667/667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 277ms/step - accuracy: 0.9496 - loss: 0.1225 - val_accuracy: 0.8922 - val_loss: 0.4320\n\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1s/step - accuracy: 0.8515 - loss: 0.4372\nModel Created and Saved Successfully!\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the model in TensorFlow SavedModel format\nconvlstm_model.save('/kaggle/working/')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import streamlit as st\nfrom PIL import Image\nimport numpy as np\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.models.load_model('/kaggle/working/convlstm_model.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Define constants\nIMAGE_HEIGHT, IMAGE_WIDTH = 64, 64\nSEQUENCE_LENGTH = 20\n\ndef preprocess_video(video_path):\n    \"\"\"Extract and preprocess frames from a video file.\"\"\"\n    video_reader = cv2.VideoCapture(video_path)\n    frames_list = []\n    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n    skip_frames_window = max(int(video_frames_count / SEQUENCE_LENGTH), 1)\n\n    for frame_counter in range(SEQUENCE_LENGTH):\n        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n        success, frame = video_reader.read()\n        if not success:\n            break\n        resized_frame = cv2.resize(frame, (IMAGE_WIDTH, IMAGE_HEIGHT))\n        normalized_frame = resized_frame / 255.0\n        frames_list.append(normalized_frame)\n\n    video_reader.release()\n    return np.array([frames_list])  # Add batch dimension","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main():\n    st.title(\"Human Activity Recognition using ConvLSTM\")\n\n    st.write(\"Upload a video to classify the human activity.\")\n\n    # File uploader\n    video_file = st.file_uploader(\"Choose a video...\", type=[\"mp4\", \"mov\", \"avi\", \"mkv\"])\n\n    if video_file is not None:\n        st.video(video_file)\n\n        # Save the uploaded video\n        with open(\"uploaded_video.mp4\", \"wb\") as f:\n            f.write(video_file.getbuffer())\n\n        # Preprocess the video\n        preprocessed_frames = preprocess_video(\"uploaded_video.mp4\")\n\n        # Predict using the model\n        predictions = model.predict(preprocessed_frames)\n        predicted_class = np.argmax(predictions, axis=1)[0]\n\n        # Class labels\n        CLASSES_LIST = ['Clapping', 'Meet and Split', 'Sitting', 'Standing Still', 'Walking', 'Walking While Reading Book', 'Walking While Using Phone']\n        st.write(f\"Predicted Activity: **{CLASSES_LIST[predicted_class]}**\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install streamlit tensorflow opencv-python numpy Pillow","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"streamlit run app.py","metadata":{},"execution_count":null,"outputs":[]}]}